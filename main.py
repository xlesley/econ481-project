# -*- coding: utf-8 -*-
"""
ECON 481 Final Project
Group9 - Yuzhi Fu, Vicky Guo, Lesley Xu
"""

"""481project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1818xhWqjJmJcSTmd1yzOCh4dCKFBlZtd
"""

from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.linear_model import LinearRegression, LassoCV
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.model_selection import cross_val_score, KFold
import statsmodels.api as sm
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.pipeline import make_pipeline
from sklearn.metrics import mean_squared_error, r2_score
from sklearn import metrics

url = "https://github.com/xlesley/econ481-project/raw/main/diamonds.csv"
df = pd.read_csv(url)
print(df.head())

"""## Data Inspection



"""

#Check for null values
print(df.isnull().sum())

#Check for duplicates
print(df.duplicated().sum())

#Delete duplicates
df = df.drop_duplicates(keep = "first")

"""## EDA"""

print(df.describe(include='all'))
print(df.info())

"""### Histogram"""

sns.set()
plt.hist(df['price'], bins=10)
plt.title('Distribution of Diamond Prices')
plt.xlabel('Price')
plt.ylabel('Frequency')
plt.show()

"""### Correlation Analysis"""

df_encoded = pd.get_dummies(df, drop_first=True)

# Correlation matrix
correlation_matrix = df_encoded.corr()

# Heatmap of the correlation matrix
plt.figure(figsize=(20, 16))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')
plt.show()

"""# Model

## Linear Regression
"""

X = df[['carat', 'cut', 'color', 'clarity', 'depth', 'table', 'x', 'y', 'z']]
y = df['price']

#Convert categorical variables into dummy/indicator variables
X = pd.get_dummies(X, drop_first=True)

# Check data types after conversion
print(X.dtypes)

# Split the data into train and test sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Apply LassoCV to find the best alpha value
lasso_cv = LassoCV(cv=5, random_state=42)
lasso_cv.fit(X_train_scaled, y_train)

# Print the best alpha value found
print(f"Best alpha value: {lasso_cv.alpha_}")

# Train the LASSO model with the best alpha value
lasso_model = lasso_cv

# Make predictions
y_train_pred = lasso_model.predict(X_train_scaled)
y_test_pred = lasso_model.predict(X_test_scaled)

# Evaluate the model
train_mse = mean_squared_error(y_train, y_train_pred)
test_mse = mean_squared_error(y_test, y_test_pred)
train_r2 = r2_score(y_train, y_train_pred)
test_r2 = r2_score(y_test, y_test_pred)

print(f"Train MSE: {train_mse}")
print(f"Test MSE: {test_mse}")
print(f"Train R2: {train_r2}")
print(f"Test R2: {test_r2}")

X = df[['carat', 'cut', 'color', 'clarity', 'x', 'y', 'z']]
y = df['price']

#Convert categorical variables into dummy/indicator variables
X = pd.get_dummies(X, drop_first=True)

# Check data types after conversion
print(X.dtypes)

# Split the data into train and test sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Add constant to the predictors for the intercept term
X_train = sm.add_constant(X_train)

# Fit the linear regression model using statsmodels
model = sm.OLS(y_train.astype(float), X_train.astype(float)).fit()

# Print the model summary
print(model.summary())

# Add constant to the predictors for the intercept term in the test dataset
X_test_const = sm.add_constant(X_test)

# Make predictions on the test dataset
y_test_pred = model.predict(X_test_const)

mse = metrics.mean_squared_error(y_test, y_test_pred)
rmse = np.sqrt(mse)
ssr = ((y_test_pred - y_test.mean())**2).sum()
sst = ((y_test - y_train.mean())**2).sum()
r2 = ssr/sst
r2

"""## Random Forest"""

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score
import statsmodels.api as sm
# Initialize the Random Forest model
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)

# Fit the model
rf_model.fit(X_train, y_train)
# Add constant term to the test predictors
X_test_const = sm.add_constant(X_test)

# Make predictions
y_train_pred_rf = rf_model.predict(X_train)
y_test_pred_rf = model.predict(X_test_const)

# Evaluate the model
train_r2_rf = r2_score(y_train, y_train_pred_rf)
test_r2_rf = r2_score(y_test, y_test_pred_rf)
print("Train R^2 Score (Random Forest):", train_r2_rf)
print("Test R^2 Score (Random Forest):", test_r2_rf)